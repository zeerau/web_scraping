{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import Select # type: ignore\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException, TimeoutException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "#from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "from fake_useragent import UserAgent  # For generating random user agents\n",
    "import pandas as pd  # For data manipulation and creating DataFrames\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "\n",
      "Web scraping completed.\n"
     ]
    }
   ],
   "source": [
    "service = Service(executable_path=r'C:\\Users\\DELL\\Documents\\web_scraping\\chromedriver-win64\\chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(\"https://fzmovies.net/\")\n",
    "driver.implicitly_wait(10)\n",
    "all_data = []\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    getting_250_list = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//ul//li//a[contains(text(),'IMDB Top 250 movies')]\"))\n",
    "\n",
    "    )\n",
    "    getting_250_list.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    while True:\n",
    "        tables = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, \"table\"))\n",
    "        )\n",
    "\n",
    "        for table in tables:\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            for row in rows[1:]: \n",
    "                second_td = row.find_elements(By.TAG_NAME, \"td\")[1]\n",
    "\n",
    "                span = second_td.find_element(By.TAG_NAME, \"span\")\n",
    "                a_element = span.find_element(By.TAG_NAME, \"a\")\n",
    "                b_element = a_element.find_element(By.TAG_NAME, \"b\")\n",
    "                small_tags_in_span = span.find_elements(By.TAG_NAME, \"small\")\n",
    "\n",
    "                small_tags_in_td = second_td.find_elements(By.TAG_NAME, \"small\")\n",
    "                rating_small = small_tags_in_td[0].find_elements(By.TAG_NAME, \"span\")  \n",
    "\n",
    "                data = pd.DataFrame({\n",
    "                \"Movie Name\": b_element.text,\n",
    "                \"small_tag_text\": small_tags_in_span[0].text,\n",
    "                \"Description\": small_tags_in_span[2].text if len(small_tags_in_span) > 2 else None,  \n",
    "                \"IMBD Rating\": rating_small[0].text if rating_small else None, \n",
    "                \"IMDB Votes\": rating_small[1].text if len(rating_small) > 1 else None,  \n",
    "            })\n",
    "\n",
    "                \n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "        EC.invisibility_of_element_located(\n",
    "            (By.CSS_SELECTOR, \n",
    "             \"a[href*='haychalk.com'], \"\n",
    "             \"a[href*='achieveweakness.com'], \"\n",
    "             \"a[href*='flusoprano.com'], \"\n",
    "             \"a[href*='tributeparticle.com'], \"\n",
    "             \"iframe[id^='container-']\"  \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div//small//a[contains(text(),'Next')]\"))\n",
    "    )\n",
    "            current_url = driver.current_url\n",
    "            try:\n",
    "                next_button.click() \n",
    "            except ElementClickInterceptedException:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "\n",
    "\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(current_url))\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Web scraping completed.\")\n",
    "\n",
    "    \n",
    "    driver.quit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
